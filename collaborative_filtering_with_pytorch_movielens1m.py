# -*- coding: utf-8 -*-
"""Collaborative Filtering with Pytorch_MovieLens1M.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R9GUym3Ygp94Um_ORKRFXtnBqxV-cHc6
"""

import pandas as pd
import numpy as np
from sklearn import model_selection, metrics, preprocessing
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Commented out IPython magic to ensure Python compatibility.
columns = ['userId', 'movieId', 'rating', 'timestamp']
df = pd.read_csv('ratings.dat', sep='::', names=columns)
print('There are %s users, %s items and %s observaciones en el dataset' \
#       %(df.userId.unique().shape[0], df.movieId.unique().shape[0], df.shape[0]))
df.head()

df.info() # Esquema básico del dataset

df.rating.value_counts() # Distribución de valores de las calificaciones

df.shape

"""#### Datos de entrenamiento"""

class MovieDataset:
    def __init__(self, users, movies, ratings):
        self.users = users
        self.movies = movies
        self.ratings = ratings
    # len(movie_dataset)
    def __len__(self):
        return len(self.users)
    # movie_dataset[1]
    def __getitem__(self, item):

        users = self.users[item]
        movies = self.movies[item]
        ratings = self.ratings[item]

        return {
            "users": torch.tensor(users, dtype=torch.long),
            "movies": torch.tensor(movies, dtype=torch.long),
            "ratings": torch.tensor(ratings, dtype=torch.long),
        }

"""#### Crear el modelo"""

class RecSysModel(nn.Module):
    def __init__(self, n_users, n_movies):
        super().__init__()
        # matriz de búsqueda entrenable para vectores de incrustación poco profundos

        self.user_embed = nn.Embedding(n_users, 32)
        self.movie_embed = nn.Embedding(n_movies, 32)
        # user, movie embedding concat
        self.out = nn.Linear(64, 1)


    def forward(self, users, movies, ratings=None):
        user_embeds = self.user_embed(users)
        movie_embeds = self.movie_embed(movies)
        output = torch.cat([user_embeds, movie_embeds], dim=1)

        output = self.out(output)

        return output

# Codificar el user y el id de la película para empezar desde 0 y no encontarnos con un índice fuera de límite con Embedding
lbl_user = preprocessing.LabelEncoder()
lbl_movie = preprocessing.LabelEncoder()

df.userId = lbl_user.fit_transform(df.userId.values)
df.movieId = lbl_movie.fit_transform(df.movieId.values)

df_train, df_valid = model_selection.train_test_split(
    df, test_size=0.1, random_state=42, stratify=df.rating.values
)

train_dataset = MovieDataset(
    users=df_train.userId.values,
    movies=df_train.movieId.values,
    ratings=df_train.rating.values
)

valid_dataset = MovieDataset(
    users=df_valid.userId.values,
    movies=df_valid.movieId.values,
    ratings=df_valid.rating.values
)

train_loader = DataLoader(dataset=train_dataset,
                          batch_size=4,
                          shuffle=True,
                          num_workers=2)

validation_loader = DataLoader(dataset=valid_dataset,
                          batch_size=4,
                          shuffle=True,
                          num_workers=2)

dataiter = iter(train_loader)
dataloader_data = next(dataiter)
print(dataloader_data)

model = RecSysModel(
    n_users=len(lbl_user.classes_),
    n_movies=len(lbl_movie.classes_),
).to(device)

optimizer = torch.optim.Adam(model.parameters())
sch = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.4)

loss_func = nn.MSELoss()

print(len(lbl_user.classes_))
print(len(lbl_movie.classes_))
print(df.movieId.max())
print(len(train_dataset))

"""#### Ejecutar manualmente una ruta de avance"""

print(dataloader_data['users'])

print(dataloader_data['users'].size())
print(dataloader_data['movies'] )
print(dataloader_data['movies'].size())

user_embed = nn.Embedding(len(lbl_user.classes_), 32)
movie_embed = nn.Embedding(len(lbl_movie.classes_), 32)

out = nn.Linear(64, 1)

user_embeds = user_embed(dataloader_data['users'])
movie_embeds = movie_embed(dataloader_data['movies'])
print(f"user_embeds {user_embeds.size()}")
print(f"user_embeds {user_embeds}")
print(f"movie_embeds {movie_embeds.size()}")
print(f"movie_embeds {movie_embeds}")

output = torch.cat([user_embeds, movie_embeds], dim=1)
print(f"output: {output.size()}")
print(f"output: {output}")
output = out(output)
print(f"output: {output}")

with torch.no_grad():
    model_output = model(dataloader_data['users'],
                   dataloader_data["movies"])

    print(f"model_output: {model_output}, size: {model_output.size()}")

rating = dataloader_data["ratings"]
print(rating)
print(rating.view(4, -1))
print(model_output)

print(rating.sum())

print(model_output.sum() - rating.sum())

"""#### Ejecutar el bucle de entrenamiento"""

epochs = 1
total_loss = 0
plot_steps, print_steps = 5000, 5000
step_cnt = 0
all_losses_list = []

history=model.train()
for epoch_i in range(epochs):
    for i, train_data in enumerate(train_loader):
        output = model(train_data["users"],
                       train_data["movies"]
                      )

        # .view(4, -1) es reformar la calificación para que coincida con la forma de la salida del modelo, que es 4x1
        rating = train_data["ratings"].view(4, 1).to(torch.float32)

        loss = loss_func(output, rating)
        total_loss = total_loss + loss.sum().item()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        step_cnt = step_cnt + len(train_data["users"])


        if(step_cnt % plot_steps == 0):
            avg_loss = total_loss/(len(train_data["users"]) * plot_steps)
            print(f"epoch {epoch_i} loss at step: {step_cnt} is {avg_loss}")
            all_losses_list.append(avg_loss)

            total_loss = 0 # reset total_loss

plt.figure()
plt.plot(all_losses_list)
plt.plot(loss_func_list, '--')
plt.title("model loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

"""##### Evaluación RMSE

"""

from sklearn.metrics import mean_squared_error

model_output_list = []
target_rating_list = []

model.eval()

with torch.no_grad():
    for i, batched_data in enumerate(validation_loader):
        model_output = model(batched_data['users'],
                       batched_data["movies"])

        model_output_list.append(model_output.sum().item() / len(batched_data['users']) )

        target_rating = batched_data["ratings"]

        target_rating_list.append(target_rating.sum().item() / len(batched_data['users']))

        print(f"model_output: {model_output}, target_rating: {target_rating}")


# al cuadrado Si es True devuelve el valor MSE, si es False devuelve el valor RMSE.
rms = mean_squared_error(target_rating_list, model_output_list, squared=False)
print(f"rms: {rms}")

"""##### Evaluación Recall@K

"""

from collections import defaultdict

# un dict que almacena una lista de pares de valoraciones previstas y reales para cada usuario
user_est_true = defaultdict(list)

# iterar a través de los datos de validación para construir el usuario-> [(y1, y1_hat), (y2, y2_hat)...]
with torch.no_grad():
    for i, batched_data in enumerate(validation_loader):
        users = batched_data['users']
        movies = batched_data['movies']
        ratings = batched_data['ratings']

        model_output = model(batched_data['users'], batched_data["movies"])

        for i in range(len(users)):
            user_id = users[i].item()
            movie_id = movies[i].item()
            pred_rating = model_output[i][0].item()
            true_rating = ratings[i].item()

            print(f"{user_id}, {movie_id}, {pred_rating}, {true_rating}")
            user_est_true[user_id].append((pred_rating, true_rating))

with torch.no_grad():
    precisions = dict()
    recalls = dict()

    k=10
    threshold=3.5

    for uid, user_ratings in user_est_true.items():

        # Ordenar las valoraciones de los usuarios por valor estimado.
        user_ratings.sort(key=lambda x: x[0], reverse=True)

        # Obtener el número real de artículos relevantes
        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)

        # Obtener el número de artículos recomendados que se predicen como relevantes y dentro del topk
        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])

        # Obtener el número de elementos comentados que' también son realmente relevantes dentro de topk
        n_rel_and_rec_k = sum(
            ((true_r >= threshold) and (est >= threshold))
            for (est, true_r) in user_ratings[:k]
        )

        print(f"uid {uid},  n_rel {n_rel}, n_rec_k {n_rec_k}, n_rel_and_rec_k {n_rel_and_rec_k}")

        # Precision@K: Proporción de elementos recomendados que son relevantes
        # Cuando n_rec_k es 0, La precisión no está definida. En este caso, la fijamos en 0.

        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0

        # Recall@K: Proporción de elementos relevantes que se recomiendan
        # Cuando n_rel es 0, Recall no está definida. Aquí lo fijamos en 0.

        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0

# Precision y recall se puede promediar entre todos los usuarios
print(f"precision @ {k}: {sum(prec for prec in precisions.values()) / len(precisions)}")

print(f"recall @ {k} : {sum(rec for rec in recalls.values()) / len(recalls)}")